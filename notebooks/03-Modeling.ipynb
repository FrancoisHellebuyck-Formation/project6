{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde38ca2",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Initiez-vous au MLOps (partie 1/2)\n",
    "\n",
    "## ü§ñ Mod√©lisation\n",
    "### üõ†Ô∏è Pr√©parez l'environnement de travail\n",
    "#### üì¶ Import des modules python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf8793b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'log_shap_artifacts' from 'src.utils_mlflow' (/Users/francoishellebuyck/Documents/projects/openclassrooms/project6/src/utils_mlflow.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisu_text\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (print_title,\n\u001b[32m     60\u001b[39m                                           print_end, \n\u001b[32m     61\u001b[39m                                           print_col,\n\u001b[32m     62\u001b[39m                                           quick_df_info,print_results)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodelization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     64\u001b[39m     print_report,\n\u001b[32m     65\u001b[39m     print_cross_validation_scores,\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils_mlflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     log_global_params_and_tags,\n\u001b[32m     69\u001b[39m     create_experiment_with_metadata,\n\u001b[32m     70\u001b[39m     start_experiment,\n\u001b[32m     71\u001b[39m     log_global_params_and_tags,\n\u001b[32m     72\u001b[39m     log_base_metrics,\n\u001b[32m     73\u001b[39m     log_complete_system_metrics,\n\u001b[32m     74\u001b[39m     log_feature_importance_with_names,\n\u001b[32m     75\u001b[39m     log_shap_artifacts\n\u001b[32m     76\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Constantes\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstantes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     79\u001b[39m     NOTEBOOK_NAME, \n\u001b[32m     80\u001b[39m     TRACKING_URI, \n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     TAGS_FINAL_MODEL,\n\u001b[32m     87\u001b[39m     EXPERIMENT)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'log_shap_artifacts' from 'src.utils_mlflow' (/Users/francoishellebuyck/Documents/projects/openclassrooms/project6/src/utils_mlflow.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "os.unsetenv('MLFLOW_TRACKING_URI')\n",
    "os.unsetenv('MLFLOW_HOME')\n",
    "\n",
    "import logging\n",
    "# R√©duire le niveau de verbosit√© de MLflow\n",
    "logging.getLogger('mlflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    make_scorer, \n",
    "    fbeta_score,\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Modules internes\n",
    "from src.models.seuil import complete_threshold_optimization\n",
    "from src.visualization.visu import plot_vertical_feature_importance\n",
    "from src.visualization.visu_text import (print_title,\n",
    "                                          print_end, \n",
    "                                          print_col,\n",
    "                                          quick_df_info,print_results)\n",
    "from src.models.modelization import (\n",
    "    print_report,\n",
    "    print_cross_validation_scores,\n",
    ")\n",
    "from src.utils_mlflow import (\n",
    "    log_global_params_and_tags,\n",
    "    create_experiment_with_metadata,\n",
    "    start_experiment,\n",
    "    log_global_params_and_tags,\n",
    "    log_base_metrics,\n",
    "    log_complete_system_metrics,\n",
    "    log_feature_importance_with_names,\n",
    "    log_shap_artifacts\n",
    ")\n",
    "# Constantes\n",
    "from src.constantes import (\n",
    "    NOTEBOOK_NAME, \n",
    "    TRACKING_URI, \n",
    "    TAGS_DEFAULT,\n",
    "    TAGS_BASELINE,\n",
    "    TAGS_CV, \n",
    "    TAGS_TO,\n",
    "    TAGS_LGBMO ,\n",
    "    TAGS_FINAL_MODEL,\n",
    "    EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c75cf",
   "metadata": {},
   "source": [
    "### Pr√©paration du jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/processed/survey_lung_cancer_features.parquet\", engine='pyarrow')\n",
    "# Conversion des colonnes bool√©ennes en int pour compatibilit√© avec certains mod√®les\n",
    "bool_cols = df.select_dtypes(include=['bool', 'boolean']).columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "# Conversion des colonnes cat√©gorielles en codes num√©riques\n",
    "cat_cols = df.select_dtypes(include=['category']).columns\n",
    "df[cat_cols] = df[cat_cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "quick_df_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad1f66",
   "metadata": {},
   "source": [
    "### ‚õìÔ∏è‚Äçüí• Separation du jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# S√âPARATION TRAIN / VALIDATION / TEST\n",
    "# ================================\n",
    "\n",
    "y = df[\"LUNG_CANCER\"]\n",
    "X = df.drop(columns=[\"LUNG_CANCER\", \"ID\"])\n",
    "\n",
    "print_title(f\"Donn√©es originales: {X.shape[0]} √©chantillons\")\n",
    "print_col(f\"Distribution: Classe 0: {(y == 0).sum()}, Classe 1: {(y == 1).sum()}\")\n",
    "print_col(f\"Proportion classe 1: {(y == 1).mean()*100:.1f}%\")\n",
    "print_end()\n",
    "\n",
    "# √âtape 1: S√©parer train+val (80%) / test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,  # 20% pour test\n",
    "    random_state=42,\n",
    "    stratify=y,  # Garde la m√™me proportion de classes\n",
    ")\n",
    "\n",
    "# √âtape 2: S√©parer train (60%) / validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.25,  # 25% de 80% = 20% du total\n",
    "    random_state=42,\n",
    "    stratify=y_temp,\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# V√âRIFICATION DES PROPORTIONS\n",
    "# ================================\n",
    "\n",
    "total = len(X)\n",
    "print_title(\"R√âPARTITION FINALE:\")\n",
    "print_col(f\" Train:      {len(X_train)} √©chantillons ({len(X_train)/total*100:.1f}%)\")\n",
    "print_col(f\" Validation: {len(X_val)} √©chantillons ({len(X_val)/total*100:.1f}%)\")\n",
    "print_col(f\" Test:       {len(X_test)} √©chantillons ({len(X_test)/total*100:.1f}%)\")\n",
    "print_end()\n",
    "\n",
    "print_title(\"V√âRIFICATION STRATIFICATION:\")\n",
    "print_col(f\" Original    - Classe 1: {(y == 1).mean()*100:.1f}%\")\n",
    "print_col(f\" Train       - Classe 1: {(y_train == 1).mean()*100:.1f}%\")\n",
    "print_col(f\" Validation  - Classe 1: {(y_val == 1).mean()*100:.1f}%\")\n",
    "print_col(f\" Test        - Classe 1: {(y_test == 1).mean()*100:.1f}%\")\n",
    "print_end()\n",
    "\n",
    "quick_df_info(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4384f",
   "metadata": {},
   "source": [
    "\n",
    "### ‚û°Ô∏è Choix des mod√®les :\n",
    "- DummyClassifier\n",
    "- LogisticRegression\n",
    "- RandomForestClassifier\n",
    "- XGBClassifier\n",
    "- LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipeline = ImbPipeline([ \n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', DummyClassifier(strategy=\"constant\", constant=1)) \n",
    "])\n",
    "rl_pipeline = ImbPipeline([ \n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(random_state=42)) \n",
    "])\n",
    "rf_pipeline = ImbPipeline([ \n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier(random_state=42)) \n",
    "])\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', xgb.XGBClassifier(random_state=42))\n",
    "])\n",
    "lgb_pipeline = ImbPipeline([\n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1  # D√©sactiver les warnings\n",
    "    ))\n",
    "])\n",
    "\n",
    "models_pipelines = {\n",
    "    \"Dummy\": dummy_pipeline,\n",
    "    \"Logistic Regression\": rl_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline,\n",
    "    \"LightGBM\": lgb_pipeline\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d33a",
   "metadata": {},
   "source": [
    "### Initialisation MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d79e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "mlflow.lightgbm.autolog(disable=True)\n",
    "mlflow.autolog(disable=True) # D√©sactivation de l'autologging pour √©viter les conflits\n",
    "# ‚ö†Ô∏è IMPORTANT : Activer AVANT start_run()\n",
    "mlflow.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ce032",
   "metadata": {},
   "source": [
    "### ü§ñ Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir l'exp√©rience (ou la cr√©er si elle n'existe pas)\n",
    "model_type=\"baseline_comparison\"\n",
    "EXPERIMENT_NAME = EXPERIMENT + \" - \" + model_type\n",
    "description=\"Exp√©rience pour comparer les mod√®les de base sur la d√©tection du cancer du poumon.\"\n",
    " \n",
    "experiment_id=start_experiment(EXPERIMENT_NAME, description, TAGS_DEFAULT | TAGS_BASELINE)\n",
    "\n",
    "model_scores = []\n",
    "for name, pipeline in models_pipelines.items():\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=f\"Run {name}\") as parent_run:\n",
    "        log_global_params_and_tags(X_train, X_test, df, TAGS_DEFAULT | TAGS_BASELINE | \n",
    "                                   {\"model_name\": name})\n",
    "        mlflow.set_tag(\"mlflow.note.content\", f\"{name} - {model_type}\")\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "\n",
    "        print_title(f\"Entra√Ænement du mod√®le: {name}\")\n",
    "        model = pipeline.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Afficher le rapport\n",
    "        accuracy, precision, recall, f1, f2 = print_report(\n",
    "            y_test, y_pred, target_names=[\"Non\", \"Oui\"]\n",
    "        )\n",
    "\n",
    "        # 1. Log des param√®tres sp√©cifiques au mod√®le (e.g., type de mod√®le)\n",
    "        mlflow.log_param(\"model_name\", name)\n",
    "        \n",
    "        # 2. Log des m√©triques (ce sont des nombres)\n",
    "        log_base_metrics( accuracy, precision, recall, f1, f2)\n",
    "\n",
    "\n",
    "        # 3. Utilise la saveur MLflow pour scikit-learn (et pour les autres mod√®les avec pipelines)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, \n",
    "            artifact_path=\"model\"\n",
    "        )\n",
    "    \n",
    "\n",
    "        model_score = {\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-score\": f1,\n",
    "            \"F2-score\": f2,\n",
    "        }\n",
    "        model_scores.append(model_score)\n",
    "        print_end()\n",
    "# Tableau des performances des mod√®les\n",
    "print_results(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f5a1b",
   "metadata": {},
   "source": [
    "### ‚ùé  Validation crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pipelines = {\n",
    "    \"Logistic Regression\": rl_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline,\n",
    "    \"LightGBM\": lgb_pipeline\n",
    "}\n",
    "model_scores = []\n",
    "\n",
    "# D√©finir l'exp√©rience (ou la cr√©er si elle n'existe pas)\n",
    "model_type=\"cross-validation\"\n",
    "EXPERIMENT_NAME = EXPERIMENT + \" - \" + model_type\n",
    "description=\"Exp√©rience de validation crois√©e pour comparer les mod√®les sur la d√©tection du cancer du poumon.\"\n",
    " \n",
    "experiment_id=start_experiment(EXPERIMENT_NAME, description, TAGS_DEFAULT | TAGS_CV)\n",
    "\n",
    "#  Validation crois√©e : Xtrain+Xtest, on utilise X_train + X_test\n",
    "X_cv = pd.concat([X_train, X_test], axis=0)\n",
    "y_cv = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "for name, pipeline in models_pipelines.items():\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=f\"Run CV {name}\") as parent_run:\n",
    "                \n",
    "        log_global_params_and_tags(X_train, X_test, df, TAGS_DEFAULT | TAGS_CV | \n",
    "                                   {\"model_name\": name})\n",
    "        mlflow.set_tag(\"mlflow.note.content\", f\"{name} - {model_type}\")\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "\n",
    "\n",
    "        # M√©triques syst√®me avant\n",
    "        print(\"üìä √âtat syst√®me avant entra√Ænement:\")\n",
    "        metrics_before = log_complete_system_metrics()\n",
    "        for key, value in metrics_before.items():\n",
    "            if 'percent' in key:\n",
    "                print(f\"   {key}: {value:.1f}%\")\n",
    "            elif 'gb' in key or 'mb' in key:\n",
    "                print(f\"   {key}: {value:.2f}\")\n",
    "    \n",
    "        mlflow.log_metrics({f\"system/{k}_before\": v for k, v in metrics_before.items()})\n",
    "    \n",
    "        # Entra√Ænement\n",
    "        print(\"\\nüèãÔ∏è Entra√Ænement en cours...\")\n",
    "        start_time = time.time()\n",
    "        accuracy, precision, recall, f1, f2, stable = (\n",
    "            print_cross_validation_scores(pipeline, X_cv, y_cv)\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        # M√©triques syst√®me apr√®s\n",
    "        print(\"\\nüìä √âtat syst√®me apr√®s entra√Ænement:\")\n",
    "        metrics_after = log_complete_system_metrics()\n",
    "        for key, value in metrics_after.items():\n",
    "            if 'percent' in key:\n",
    "                print(f\"   {key}: {value:.1f}%\")\n",
    "        \n",
    "        mlflow.log_metrics({f\"system/{k}_after\": v for k, v in metrics_after.items()})\n",
    "        \n",
    "        # M√©triques de performance\n",
    "        mlflow.log_metrics({\n",
    "            'system/training_time_seconds': training_time,\n",
    "            'system/training_time_minutes': training_time / 60,\n",
    "        })\n",
    "\n",
    "\n",
    "         # 1. Log des param√®tres sp√©cifiques au mod√®le (e.g., type de mod√®le)\n",
    "        mlflow.log_param(\"model_name\", name)\n",
    "        \n",
    "        # 2. Log des m√©triques (ce sont des nombres)\n",
    "        log_base_metrics( accuracy, precision, recall, f1, f2)\n",
    "        mlflow.log_param(\"stability\", stable)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, \n",
    "            artifact_path=\"models\"\n",
    "        )\n",
    "    \n",
    "        model_score = {\n",
    "                    \"Model\": name,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1-score\": f1,\n",
    "                    \"F2-score\": f2,\n",
    "                    \"Stabilit√©\": stable,\n",
    "                }\n",
    "        model_scores.append(model_score)\n",
    "# Tableau des performances des mod√®les\n",
    "print_results(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7fbfe",
   "metadata": {},
   "source": [
    "### üìà Calcul des seuils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le sans param√®tres optimis√©s\n",
    "models_pipelines = {\n",
    "    \"Logistic Regression\": rl_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline,\n",
    "    \"LightGBM\": lgb_pipeline\n",
    "}\n",
    "model_scores = []\n",
    "# D√©finir l'exp√©rience (ou la cr√©er si elle n'existe pas)\n",
    "model_type=\"threshold_optimization\"\n",
    "EXPERIMENT_NAME = EXPERIMENT + \" - \" + model_type\n",
    "description=\"Optimisation du seuil de d√©cision pour maximiser le F2-score sur la d√©tection du cancer du poumon.\"\n",
    " \n",
    "experiment_id=start_experiment(EXPERIMENT_NAME, description, TAGS_DEFAULT | TAGS_TO)\n",
    "\n",
    "\n",
    "for name, pipeline in models_pipelines.items():\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=f\"Run threshold {name}\") as parent_run:\n",
    "\n",
    "        log_global_params_and_tags(X_train, X_test, df, TAGS_DEFAULT | TAGS_TO | \n",
    "                                   {\"model_name\": name})\n",
    "        mlflow.set_tag(\"mlflow.note.content\", f\"{name} - {model_type}\")\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "\n",
    "         # 1. Entra√Æner le mod√®le sur train\n",
    "\n",
    "        print_title(f\"Optimisation du seuil pour le mod√®le: {name}\")\n",
    "        # Entra√Ænement\n",
    "        model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # 2. Optimiser le seuil sur validation\n",
    "        optimal_threshold, eval_results = complete_threshold_optimization(model, X_val, y_val)\n",
    "\n",
    "        # 3. √âvaluation finale sur test avec seuil optimal\n",
    "        y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_test_pred = (y_test_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "        final_f2 = fbeta_score(y_test, y_test_pred, beta=2)\n",
    "        print_col(\"\")\n",
    "        print_col(f\"F2-Score final sur test: {final_f2:.4f}\")\n",
    "        print_end()\n",
    "\n",
    "        accuracy, precision, recall, f1, f2 = print_report(\n",
    "            y_test, y_test_pred, target_names=[\"Non\", \"Oui\"]\n",
    "        )\n",
    "        model_score = {\n",
    "                \"Model\": name,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1-score\": f1,\n",
    "                \"F2-score\": f2,\n",
    "                \"Optimal Threshold\": optimal_threshold,\n",
    "            }\n",
    "        log_base_metrics( accuracy, precision, recall, f1, f2)\n",
    "        mlflow.log_metric(\"Optimal Threshold\", optimal_threshold)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline, \n",
    "            artifact_path=\"models\"\n",
    "        )\n",
    "        model_scores.append(model_score)\n",
    "        print_end()\n",
    "# Tableau des performances des mod√®les\n",
    "print_results(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5942c1",
   "metadata": {},
   "source": [
    "### üßÆ Hyperparameter LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer tous les runs actifs s'il y en a\n",
    "while mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "print(\"‚úÖ Tous les runs actifs ont √©t√© ferm√©s\")\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        objective='multiclass',\n",
    "        num_class=3,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Grille\n",
    "param_grid = {\n",
    "    'classifier__num_leaves': [15, 31, 63],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [5, 10, -1]\n",
    "}\n",
    "\n",
    "# === SCORING AVEC F2 ===\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, average='weighted')\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision_weighted',\n",
    "    'recall': 'recall_weighted',\n",
    "    'f1': 'f1_weighted',\n",
    "    'f2': f2_scorer\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    refit='f2',  # Optimiser sur F2 (crucial pour d√©tection m√©dicale)\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# D√©finir l'exp√©rience (ou la cr√©er si elle n'existe pas)\n",
    "model_type=\"LGBM_optimization\"\n",
    "EXPERIMENT_NAME = EXPERIMENT + \" - \" + model_type\n",
    "description=\"Optimisation du mod√®le LGBM.\"\n",
    " \n",
    "experiment_id=start_experiment(EXPERIMENT_NAME, description, TAGS_DEFAULT | TAGS_LGBMO)\n",
    "\n",
    "    \n",
    "print(\"üîç GridSearch optimis√© pour F2 score...\")\n",
    "\n",
    "# Entra√Æner\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=f\"LGBM-optimization\") as parent_run:\n",
    "    # === LOGGER LES MEILLEURS R√âSULTATS ===\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_param(\"optimized_on\", \"f2_score\")\n",
    "\n",
    "    # Logger les meilleurs scores CV pour chaque m√©trique\n",
    "    print(f\"\\nüìä Best CV Scores:\")\n",
    "    for metric in scoring.keys():\n",
    "        best_score = grid_search.cv_results_[f'mean_test_{metric}'][grid_search.best_index_]\n",
    "        mlflow.log_metric(f\"best_cv_{metric}\", best_score)\n",
    "        print(f\"   {metric}: {best_score:.4f}\")\n",
    "\n",
    "    # === √âVALUATION TEST ===\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    test_metrics = {\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "        'test_precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'test_recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'test_f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'test_f2': fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
    "    }\n",
    "\n",
    "    mlflow.log_metrics(test_metrics)\n",
    "\n",
    "    print(f\"\\nüìà Test Metrics:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "    # === LOGGER LE MOD√àLE ===\n",
    "    mlflow.sklearn.log_model(grid_search.best_estimator_, \"best_model\")\n",
    "\n",
    "    # === LOGGER LES CONFIGURATIONS (NESTED RUNS) ===\n",
    "    cv_results = grid_search.cv_results_\n",
    "\n",
    "    print(f\"\\nüìù Logging {len(cv_results['params'])} configurations...\")\n",
    "\n",
    "    for i in range(len(cv_results['params'])):\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"config_{i}\", nested=True):\n",
    "            \n",
    "            # Param√®tres\n",
    "            params = cv_results['params'][i]\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "            # M√©triques pour toutes les m√©triques de scoring\n",
    "            for metric in scoring.keys():\n",
    "                mlflow.log_metrics({\n",
    "                    f'mean_test_{metric}': cv_results[f'mean_test_{metric}'][i],\n",
    "                    f'std_test_{metric}': cv_results[f'std_test_{metric}'][i],\n",
    "                    f'mean_train_{metric}': cv_results[f'mean_train_{metric}'][i]\n",
    "                })\n",
    "            \n",
    "            # Rank\n",
    "            mlflow.log_metric(\"rank_test_f2\", cv_results['rank_test_f2'][i])\n",
    "            \n",
    "            # Tags\n",
    "            mlflow.set_tag(\"is_best\", i == grid_search.best_index_)\n",
    "            mlflow.set_tag(\"config_id\", i)\n",
    "\n",
    "    # === TAGS ET DESCRIPTION ===\n",
    "    mlflow.set_tags({\n",
    "        'model_type': 'LightGBM',\n",
    "        'search_method': 'GridSearchCV',\n",
    "        'optimization_metric': 'F2',\n",
    "        'dataset': 'lung_cancer'\n",
    "    })\n",
    "\n",
    "\n",
    "    results=f\"\"\"\n",
    "        # GridSearch Results - Optimized for F2 Score\n",
    "        \n",
    "        ## Why F2?\n",
    "        F2 score gives **2x more weight to recall** than precision.\n",
    "        This is crucial for medical detection where **missing a positive case (false negative) is much worse than a false positive**.\n",
    "        \n",
    "        ## Best Parameters\n",
    "    ```\n",
    "        {grid_search.best_params_}\n",
    "    ```\n",
    "        \n",
    "        ## Best CV Scores\n",
    "        - Accuracy: {cv_results['mean_test_accuracy'][grid_search.best_index_]:.4f}\n",
    "        - Precision: {cv_results['mean_test_precision'][grid_search.best_index_]:.4f}\n",
    "        - Recall: {cv_results['mean_test_recall'][grid_search.best_index_]:.4f}\n",
    "        - F1: {cv_results['mean_test_f1'][grid_search.best_index_]:.4f}\n",
    "        - **F2: {cv_results['mean_test_f2'][grid_search.best_index_]:.4f}**\n",
    "        \n",
    "        ## Test Performance\n",
    "        - Accuracy: {test_metrics['test_accuracy']:.4f}\n",
    "        - Precision: {test_metrics['test_precision']:.4f}\n",
    "        - Recall: {test_metrics['test_recall']:.4f}\n",
    "        - F1: {test_metrics['test_f1']:.4f}\n",
    "        - **F2: {test_metrics['test_f2']:.4f}**\n",
    "        \"\"\"\n",
    "\n",
    "    mlflow.set_tag(\n",
    "        \"mlflow.note.content\", results\n",
    "    )\n",
    "\n",
    "\n",
    "    print_title(\"R√âSULTATS FINAUX DU GRIDSEARCH\")\n",
    "    print(results)\n",
    "    print_end()\n",
    "\n",
    "    print(f\"\\n‚úÖ GridSearch complet avec F2!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f64b6",
   "metadata": {},
   "source": [
    "### Mod√®le Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer tous les runs actifs s'il y en a\n",
    "while mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "print(\"‚úÖ Tous les runs actifs ont √©t√© ferm√©s\")\n",
    "\n",
    "# Cr√©er un ColumnTransformer qui pr√©serve les noms\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), X_train.columns.tolist())\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline avec noms pr√©serv√©s\n",
    "pipeline_with_names = ImbPipeline([\n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        objective='multiclass',\n",
    "        num_class=3,\n",
    "        verbose=-1,\n",
    "        **grid_search.best_params_\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_final = ImbPipeline([\n",
    "    ('oversampling', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        **grid_search.best_params_\n",
    "    ))\n",
    "])\n",
    "\n",
    "# D√©finir l'exp√©rience (ou la cr√©er si elle n'existe pas)\n",
    "model_type=\"LGBM_Final_model\"\n",
    "EXPERIMENT_NAME = EXPERIMENT + \" - \" + model_type\n",
    "description=\"Mod√®le final optimis√© LGBM avec les meilleurs param√®tres.\"\n",
    " \n",
    "experiment_id=start_experiment(EXPERIMENT_NAME, description, TAGS_DEFAULT | TAGS_FINAL_MODEL)\n",
    "\n",
    "model_scores = []\n",
    "log_global_params_and_tags(X_train, X_test, df, TAGS_DEFAULT | TAGS_FINAL_MODEL | \n",
    "                                   {\"model_name\": name})\n",
    "mlflow.set_tag(\"mlflow.note.content\", f\"{name} - {model_type}\")\n",
    "mlflow.log_param(\"model_type\", model_type)\n",
    "             \n",
    "# Logger les meilleurs hyperparam√®tres du GridSearch\n",
    "mlflow.log_params(grid_search.best_params_)\n",
    "mlflow.log_param(\"source\", \"gridsearch_optimization\")\n",
    "\n",
    "print(\"üîç Entra√Ænement du mod√®le final avec les meilleurs param√®tres...\")\n",
    "start_time = time.time()\n",
    "model_final = pipeline_final.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "mlflow.log_metric(\"system/training_time_seconds\", training_time)\n",
    "y_pred = model_final.predict(X_test)\n",
    "# Afficher le rapport\n",
    "accuracy, precision, recall, f1, f2 = print_report(\n",
    "    y_test, y_pred, target_names=[\"Non\", \"Oui\"]\n",
    ")\n",
    "\n",
    "mlflow.log_metrics({\n",
    "        'initial_accuracy': accuracy,\n",
    "        'initial_precision': precision,\n",
    "        'initial_recall': recall,\n",
    "        'initial_f1': f1,\n",
    "        'initial_f2': f2\n",
    "    })\n",
    "\n",
    " # === CROSS-VALIDATION ===\n",
    "\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1, cv_f2, stable = (\n",
    "        print_cross_validation_scores(pipeline_final, X_train, y_train)\n",
    "    )\n",
    "mlflow.log_metrics({\n",
    "        'cv_accuracy': cv_accuracy,\n",
    "        'cv_precision': cv_precision,\n",
    "        'cv_recall': cv_recall,\n",
    "        'cv_f1': cv_f1,\n",
    "        'cv_f2': cv_f2,\n",
    "    })\n",
    "mlflow.log_param(\"stability\", stable)\n",
    "\n",
    "# === OPTIMISATION DU THRESHOLD ===\n",
    "\n",
    "optimal_threshold, eval_results = complete_threshold_optimization(model_final, X_val, y_val)\n",
    "y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = (y_test_pred_proba >= optimal_threshold).astype(int)\n",
    "mlflow.log_param(\"optimal_threshold\", optimal_threshold)\n",
    "# Logger les r√©sultats de l'optimisation du threshold si disponibles\n",
    "if eval_results:\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            mlflow.log_metric(f\"threshold_opt_{key}\", value)\n",
    "\n",
    "\n",
    "final_f2 = fbeta_score(y_test, y_test_pred, beta=2)\n",
    "print_col(\"\")\n",
    "print_col(f\"F2-Score final sur test: {final_f2:.4f}\")\n",
    "print_end()\n",
    "\n",
    "# Rapport final\n",
    "final_accuracy, final_precision, final_recall, final_f1, final_f2 = print_report(\n",
    "    y_test, y_test_pred, target_names=[\"Non\", \"Oui\"]\n",
    ")\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    'final_accuracy': final_accuracy,\n",
    "    'final_precision': final_precision,\n",
    "    'final_recall': final_recall,\n",
    "    'final_f1': final_f1,\n",
    "    'final_f2': final_f2\n",
    "})\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non\", \"Oui\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Final Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_final.png', dpi=100, bbox_inches='tight')\n",
    "mlflow.log_artifact('confusion_matrix_final.png')\n",
    "plt.close()\n",
    "os.remove('confusion_matrix_final.png')\n",
    "\n",
    "\n",
    "print(\"\\nüéØ Feature Importance...\")\n",
    "    \n",
    "fi_df = log_feature_importance_with_names(\n",
    "        pipeline=pipeline_final,\n",
    "        X_train=X_train,\n",
    "        mlflow_enabled=True\n",
    "    )\n",
    "\n",
    "\n",
    "# === TAGS ===\n",
    "tags = {\n",
    "    'model_type': 'LightGBM',\n",
    "    'pipeline': 'SMOTE + StandardScaler + LightGBM',\n",
    "    'optimization': 'GridSearch + Threshold',\n",
    "    'status': 'production_ready',\n",
    "    'threshold_optimized': 'yes'}\n",
    "\n",
    "\n",
    "model_metadata = {\n",
    "    \"description\": \"Mod√®le final optimis√© pour la d√©tection de cancer\",\n",
    "    \"author\": \"Fran√ßois Hellebuyck\",\n",
    "    **tags\n",
    "}\n",
    "\n",
    "signature = infer_signature(X_train, y_test_pred_proba)\n",
    "\n",
    "mlflow.sklearn.log_model(\n",
    "    sk_model=model_final,\n",
    "    artifact_path=\"final_model\",\n",
    "    signature=signature,\n",
    "    input_example=X_train[:5],\n",
    "    registered_model_name=\"LungCancer_LightGBM_Final\",\n",
    "    metadata=model_metadata\n",
    ")\n",
    "\n",
    "mlflow.set_tags(tags)\n",
    "\n",
    "log_shap_artifacts(model_final, X_test, classifier_name=\"classifier\", \n",
    "                   scaler_name=\"scaler\", prefix=\"final_model_shap\")\n",
    "\n",
    "# === DESCRIPTION D√âTAILL√âE ===\n",
    "description_text = f\"\"\"\n",
    "# Mod√®le Final de D√©tection de Cancer du Poumon\n",
    "\n",
    "## Pipeline\n",
    "1. **SMOTE** - Gestion du d√©s√©quilibre des classes\n",
    "2. **StandardScaler** - Normalisation des features\n",
    "3. **LightGBM** - Classificateur optimis√©\n",
    "\n",
    "## Hyperparam√®tres (issus de GridSearch)\n",
    "```python\n",
    "{grid_search.best_params_}\n",
    "```\n",
    "\n",
    "## Optimisation du Threshold\n",
    "- **Threshold optimal**: {optimal_threshold:.4f}\n",
    "- Optimis√© sur le set de validation pour maximiser le F2-score\n",
    "\n",
    "## Performance Cross-Validation (5-fold)\n",
    "- Accuracy: {cv_accuracy:.4f}\n",
    "- Precision: {cv_precision:.4f}\n",
    "- Recall: {cv_recall:.4f}\n",
    "- F1: {cv_f1:.4f}\n",
    "- **F2**: {cv_f2:.4f}\n",
    "- Stabilit√©: {stable}\n",
    "\n",
    "## Performance Test Set (avec threshold par d√©faut 0.5)\n",
    "- Accuracy: {accuracy:.4f}\n",
    "- Precision: {precision:.4f}\n",
    "- Recall: {recall:.4f}\n",
    "- F1: {f1:.4f}\n",
    "- F2: {f2:.4f}\n",
    "\n",
    "## Performance Test Set (avec threshold optimis√© {optimal_threshold:.4f})\n",
    "- Accuracy: {final_accuracy:.4f}\n",
    "- Precision: {final_precision:.4f}\n",
    "- Recall: {final_recall:.4f}\n",
    "- F1: {final_f1:.4f}\n",
    "- **F2**: {final_f2:.4f}\n",
    "\n",
    "## Am√©lioration gr√¢ce au threshold\n",
    "- Œî F2: {final_f2 - f2:+.4f}\n",
    "- Œî Recall: {final_recall - recall:+.4f}\n",
    "- Œî Precision: {final_precision - precision:+.4f}\n",
    "\n",
    "## Temps d'entra√Ænement\n",
    "- {training_time:.2f} secondes\n",
    "\n",
    "## Top 5 Features\n",
    "{fi_df.head(5)[['feature', 'importance']].to_string(index=False)}\n",
    "\n",
    "---\n",
    "**Mod√®le pr√™t pour la production** üöÄ\n",
    "\"\"\"\n",
    "    \n",
    "mlflow.set_tag(\"mlflow.note.content\", description_text)\n",
    "\n",
    "\n",
    "# Fermer tous les runs actifs s'il y en a\n",
    "while mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab06373",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
